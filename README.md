# ğŸ•“ Temporal Robustness Benchmarking of LLMs

This repository contains our custom benchmarking framework designed to test and evaluate the temporal reasoning abilities of Large Language Models (LLMs) like GPT-4, Gemini, and LLaMA.

> â— Unlike standard benchmarks like ComplexTempQA, our goal is to go beyond timestamp recall and assess temporal consistency, multi-hop reasoning, and robustness against contradictions.

---

## ğŸ“Œ Project Goals

- Evaluate how well LLMs handle:
  - Temporal ordering
  - Overlapping and parallel events
  - Indirect durations and counterfactuals
  - Temporal contradictions and hallucinations
- Stress test models using narrative-style fact sets that conflict with real-world timelines.
- Analyze failure modes such as:
  - Memorization bias
  - Confusion in ambiguous ranges (e.g., "early 1980s")
  - Logical inconsistency across question types

---

## ğŸš€ Benchmark Tool Features

ğŸ§  Built with Streamlit, our interactive tool supports:

- ğŸ§ª Multi-model testing: GPT-4, Gemini, LLaMA (Groq), Claude
- ğŸ’¬ Multiple question formats:
  - Yes/No
  - Event Order
  - Duration & Range
  - Contradiction Detection
  - Fill-in-the-blank
- ğŸ” Annotation + tagging of model failures (hallucination, contradiction, etc.)
- ğŸ“Š Logging of responses, verdicts, and error patterns
- ğŸ“ Structured test cases in JSON with:
  - System prompt
  - Synthetic fact set
  - 10â€“20 question variants

---

## ğŸ“‚ Repo Structure

ğŸ“¦ Temporal-Robustness-Benchmarking-LLMs/
â”œâ”€â”€ streamlit_app/             # Frontend interface for model testing
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ prompts/
â”œâ”€â”€ testcases/                 # JSON files with synthetic fact sets & questions
â”œâ”€â”€ results/                   # CSV logs of model responses and verdicts
â”œâ”€â”€ analysis/                  # Scripts for error heatmaps and aggregation
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
